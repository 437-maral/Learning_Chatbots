{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/masoumehvojood/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/masoumehvojood/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large-cnn and are newly initialized: ['model.shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/05/m0z3j2y1647d9pk4d3932ks80000gn/T/ipykernel_57898/4041424415.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
      "/var/folders/05/m0z3j2y1647d9pk4d3932ks80000gn/T/ipykernel_57898/4041424415.py:25: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  prompt_to_LLAMA2 = LLMChain(llm=llm, prompt=pt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/masoumehvojood/Library/Python/3.9/lib/python/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from transformers import pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "hf_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# 2. Wrap it for LangChain\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "temp = \"\"\"\n",
    "<s><<SYS>>\n",
    "List the key points with details from the context: \n",
    "[INST] The context : {context} [/INST] \n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "pt = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template= temp)\n",
    "prompt_to_LLAMA2 = LLMChain(llm=llm, prompt=pt)\n",
    "\n",
    "def transcript_audio(audio_file):\n",
    "    # Initialize the speech recognition pipeline\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=\"openai/whisper-tiny.en\",\n",
    "        chunk_length_s=30,\n",
    "        )\n",
    "    \n",
    "    # Transcribe the audio file and return the result\n",
    "    transcript_txt = pipe(audio_file, batch_size=8)[\"text\"]\n",
    "    # run the chain to merge transcript text with the template and send it to the LLM\n",
    "    result = prompt_to_LLAMA2.run(transcript_txt) \n",
    "    return result\n",
    "# Set up Gradio interface\n",
    "audio_input = gr.Audio(sources=\"upload\", type=\"filepath\")  # Audio input\n",
    "output_text = gr.Textbox()  # Text output\n",
    "# Create the Gradio interface with the function, inputs, and outputs\n",
    "iface = gr.Interface(fn=transcript_audio, \n",
    "                     inputs=audio_input, outputs=output_text, \n",
    "                     title=\"Voice Summarization\",\n",
    "                     description=\"Upload the audio file\")\n",
    "# Launch the Gradio app\n",
    "iface.launch(server_name=\"0.0.0.0\", server_port=7860)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
